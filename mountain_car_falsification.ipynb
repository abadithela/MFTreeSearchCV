{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC,SVR\n",
    "import os\n",
    "import sys\n",
    "from MFTreeSearchCV.MFTreeSearchCV import *\n",
    "from mf.mf_func import *\n",
    "import scipy as sp\n",
    "import scipy.io as scio\n",
    "import pickle as pkl\n",
    "from multipolyfit.multipolyfit import multipolyfit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import linear_model \n",
    "import matlab\n",
    "import matlab.engine as meng\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9726920303599369"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start MATLAB through MATLAB Engine API\n",
    "eng = meng.connect_matlab()\n",
    "eng.cd('../mountain_car/')# Location of simulator\n",
    "\n",
    "# setup:\n",
    "meng.find_matlab()\n",
    "eng.setup_mc(nargout=0)\n",
    "eng.query_simulator(-0.52, -0.0297) # Sanity check, Answer should be around -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Load Data: All of it\n",
    "RHO_mat = scio.loadmat('data/mountain_car/RHO.mat')\n",
    "X0_mat = scio.loadmat('data/mountain_car/X0.mat')\n",
    "V0_mat = scio.loadmat('data/mountain_car/V0.mat')\n",
    "\n",
    "RHO = RHO_mat['RHO']\n",
    "X0 = X0_mat['X0']\n",
    "V0 = V0_mat['V0']\n",
    "\n",
    "# Data for violations:\n",
    "rho_r_mat = scio.loadmat('data/mountain_car/rho_r.mat')\n",
    "xr_mat = scio.loadmat('data/mountain_car/xr.mat')\n",
    "vr_mat = scio.loadmat('data/mountain_car/vr.mat')\n",
    "\n",
    "rho_r = rho_r_mat['rho_r']\n",
    "xr = xr_mat['xr']\n",
    "vr = vr_mat['vr']\n",
    "\n",
    "# Data for successes:\n",
    "rho_g_mat = scio.loadmat('data/mountain_car/rho_g.mat')\n",
    "xg_mat = scio.loadmat('data/mountain_car/xg.mat')\n",
    "vg_mat = scio.loadmat('data/mountain_car/vg.mat')\n",
    "\n",
    "rho_g = rho_g_mat['rho_g']\n",
    "xg = xg_mat['xg']\n",
    "vg = vg_mat['vg']\n",
    "\n",
    "Nsamples = len(X0.T)\n",
    "assert(Nsamples == len(V0.T))\n",
    "assert(Nsamples*Nsamples == len(RHO[:,0]))\n",
    "print(Nsamples)\n",
    "DEG = 4 # Degree of the polynomial that is being fit to the data\n",
    "SET_APPROX = 0\n",
    "disc_z = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9] # discretizing z\n",
    "step = 0.1\n",
    "ndisc = len(disc_z)\n",
    "print(ndisc)\n",
    "fidel_dim = 2\n",
    "\n",
    "# Regressors:\n",
    "classifiers = [\n",
    "    SVR(),\n",
    "    linear_model.SGDRegressor(),\n",
    "    linear_model.BayesianRidge(),\n",
    "    linear_model.LassoLars(),\n",
    "    linear_model.ARDRegression(),\n",
    "    linear_model.PassiveAggressiveRegressor(),\n",
    "    linear_model.TheilSenRegressor(),\n",
    "    linear_model.LinearRegression()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing function approximations using polyfit:\n",
    "fname_approx_polyfit = \"Approximations/multipolyfit_approx.pkl\"\n",
    "fname_approx_logreg = \"Approximations/logreg_approx.pkl\"\n",
    "\n",
    "rsv_approxs_polyfit = dict() # List containing poly-fit functions that approximate the robust satisfaction value\n",
    "rsv_approxs_logreg = dict() # List containing poly-fit functions that approximate the robust satisfaction value\n",
    "    \n",
    "def construct_approx_polyfit():\n",
    "    for x0_idx in disc_z:\n",
    "        for v0_idx in disc_z: \n",
    "            idx = ndisc*(x0_idx*10 - 1) + v0_idx*10\n",
    "            x0_num = int((x0_idx + step)*Nsamples)\n",
    "            v0_num = int((v0_idx + step)*Nsamples)\n",
    "            assert(x0_num <= Nsamples)\n",
    "            if(v0_num > Nsamples):\n",
    "                print(v0_num)\n",
    "                assert(v0_num <= Nsamples)\n",
    "            x0_indices = np.random.choice(Nsamples, x0_num, replace=False)\n",
    "            v0_indices = np.random.choice(Nsamples, v0_num, replace=False)\n",
    "            rho0_indices = [Nsamples*(x_ii-1)+v_ii  for x_ii in x0_indices for v_ii in v0_indices]\n",
    "            x0_samples = [X0.T[ii,0] for ii in x0_indices] # Indices for x0 from the original data\n",
    "            v0_samples = [V0.T[ii,0] for ii in v0_indices] # Indices for v0 from the original data\n",
    "            assert(len(rho0_indices) == len(x0_indices)*len(v0_indices))\n",
    "            x0 = np.array([[xi, vi] for xi in x0_samples for vi in v0_samples]) # Combined x0 and v0\n",
    "            rho = np.array([RHO[ii,0] for ii in rho0_indices])\n",
    "            assert(len(x0) == len(rho))\n",
    "            \n",
    "            model = multipolyfit(x0, rho, DEG, model_out = True)\n",
    "            print(model)\n",
    "            rsv_approxs_polyfit[idx] = model\n",
    "    # pkl.dump(rsv_approxs, open(fname_approx_polyfit, \"wb\"))\n",
    "\n",
    "zidx = lambda z1,z2: ndisc*(z1*10)+z2*10\n",
    "# Constructing function approximations via tree search:\n",
    "n_x0_samples = []\n",
    "n_v0_samples = []\n",
    "def construct_approx_logreg():\n",
    "    for x0_idx in disc_z:\n",
    "        for v0_idx in disc_z:\n",
    "            idx = zidx(x0_idx, v0_idx)\n",
    "            x0_num = int((x0_idx + step)*Nsamples)\n",
    "            v0_num = int((v0_idx + step)*Nsamples)\n",
    "            assert(x0_num <= Nsamples)\n",
    "            if(v0_num > Nsamples):\n",
    "                print(v0_num)\n",
    "                assert(v0_num <= Nsamples)\n",
    "            x0_indices = np.random.choice(Nsamples, x0_num, replace=False)\n",
    "            v0_indices = np.random.choice(Nsamples, v0_num, replace=False)\n",
    "            rho0_indices = [Nsamples*(x_ii-1)+v_ii  for x_ii in x0_indices for v_ii in v0_indices]\n",
    "            x0_samples = [X0.T[ii,0] for ii in x0_indices] # Indices for x0 from the original data\n",
    "            v0_samples = [V0.T[ii,0] for ii in v0_indices] # Indices for v0 from the original data\n",
    "            n_x0_samples.append(len(x0_samples))\n",
    "            n_v0_samples.append(len(v0_samples))\n",
    "            assert(len(rho0_indices) == len(x0_indices)*len(v0_indices))\n",
    "            x0 = np.array([[xi, vi] for xi in x0_samples for vi in v0_samples]) # Combined x0 and v0\n",
    "            rho = np.array([RHO[ii,0] for ii in rho0_indices])\n",
    "            assert(len(x0) == len(rho))\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(x0, rho)\n",
    "            reg = linear_model.SGDRegressor()\n",
    "            reg.fit(X_train, y_train)\n",
    "            print(reg.score(X_test, y_test))\n",
    "            rsv_approxs_logreg[int(idx)] = reg\n",
    "    # pkl.dump(rsv_approxs, open(fname_approx_logreg, \"wb\"))\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check:\n",
    "xopt = [-0.52, -0.0297]\n",
    "classif = classifiers.copy()\n",
    "xopt_arr = np.array(xopt).reshape(1,-1)\n",
    "x0_idx = 0.1\n",
    "v0_idx = 0.1\n",
    "rsv_approxs_logreg = dict()\n",
    "idx = zidx(x0_idx, v0_idx)\n",
    "print(int(idx))\n",
    "x0_num = int((x0_idx + step)*Nsamples)\n",
    "v0_num = int((v0_idx + step)*Nsamples)\n",
    "assert(x0_num <= Nsamples)\n",
    "if(v0_num > Nsamples):\n",
    "    print(v0_num)\n",
    "    assert(v0_num <= Nsamples)\n",
    "x0_indices = np.random.choice(Nsamples, x0_num, replace=False)\n",
    "v0_indices = np.random.choice(Nsamples, v0_num, replace=False)\n",
    "rho0_indices = [Nsamples*(x_ii-1)+v_ii  for x_ii in x0_indices for v_ii in v0_indices]\n",
    "x0_samples = [X0.T[ii,0] for ii in x0_indices] # Indices for x0 from the original data\n",
    "v0_samples = [V0.T[ii,0] for ii in v0_indices] # Indices for v0 from the original data\n",
    "assert(len(rho0_indices) == len(x0_indices)*len(v0_indices))\n",
    "x0 = np.array([[xi, vi] for xi in x0_samples for vi in v0_samples]) # Combined x0 and v0\n",
    "rho = np.array([RHO[ii,0] for ii in rho0_indices])\n",
    "assert(len(x0) == len(rho))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x0, rho)\n",
    "print(len(X_train))\n",
    "reg1 = classif[-1]\n",
    "reg1.fit(X_train, y_train)\n",
    "print(reg1.score(X_test, y_test))\n",
    "rsv_approxs_logreg[int(idx)] = reg1\n",
    "print(reg1.predict(xopt_arr))\n",
    "\n",
    "\n",
    "# ssecond one:\n",
    "x0_idx = 0.9\n",
    "v0_idx = 0.5\n",
    "classif = classifiers.copy()\n",
    "idx = zidx(x0_idx, v0_idx)\n",
    "print(int(idx))\n",
    "x0_num = int((x0_idx + step)*Nsamples)\n",
    "v0_num = int((v0_idx + step)*Nsamples)\n",
    "assert(x0_num <= Nsamples)\n",
    "if(v0_num > Nsamples):\n",
    "    print(v0_num)\n",
    "    assert(v0_num <= Nsamples)\n",
    "x0_indices = np.random.choice(Nsamples, x0_num, replace=False)\n",
    "v0_indices = np.random.choice(Nsamples, v0_num, replace=False)\n",
    "rho0_indices = [Nsamples*(x_ii-1)+v_ii  for x_ii in x0_indices for v_ii in v0_indices]\n",
    "x0_samples = [X0.T[ii,0] for ii in x0_indices] # Indices for x0 from the original data\n",
    "v0_samples = [V0.T[ii,0] for ii in v0_indices] # Indices for v0 from the original data\n",
    "assert(len(rho0_indices) == len(x0_indices)*len(v0_indices))\n",
    "x0 = np.array([[xi, vi] for xi in x0_samples for vi in v0_samples]) # Combined x0 and v0\n",
    "rho = np.array([RHO[ii,0] for ii in rho0_indices])\n",
    "assert(len(x0) == len(rho))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x0, rho)\n",
    "print(len(X_train))\n",
    "reg2 = classif[-1]\n",
    "reg2.fit(X_train, y_train)\n",
    "print(reg2.score(X_test, y_test))\n",
    "rsv_approxs_logreg[int(idx)] = reg2\n",
    "\n",
    "\n",
    "print(reg2.predict(xopt_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function mk_model.<locals>.model at 0x116b23950>\n",
      "<function mk_model.<locals>.model at 0x116b23598>\n",
      "<function mk_model.<locals>.model at 0x116b23a60>\n",
      "<function mk_model.<locals>.model at 0x116b23620>\n",
      "<function mk_model.<locals>.model at 0x116b236a8>\n",
      "<function mk_model.<locals>.model at 0x116b23ae8>\n",
      "<function mk_model.<locals>.model at 0x116b23510>\n",
      "<function mk_model.<locals>.model at 0x116b239d8>\n",
      "<function mk_model.<locals>.model at 0x116b232f0>\n",
      "<function mk_model.<locals>.model at 0x116b237b8>\n",
      "<function mk_model.<locals>.model at 0x11c1ce158>\n",
      "<function mk_model.<locals>.model at 0x11c1ce0d0>\n",
      "<function mk_model.<locals>.model at 0x11c1ce048>\n",
      "<function mk_model.<locals>.model at 0x11c1ce1e0>\n",
      "<function mk_model.<locals>.model at 0x11c1ce268>\n",
      "<function mk_model.<locals>.model at 0x11c1ce2f0>\n",
      "<function mk_model.<locals>.model at 0x11c1ce400>\n",
      "<function mk_model.<locals>.model at 0x11c1ce488>\n",
      "<function mk_model.<locals>.model at 0x11c1ce510>\n",
      "<function mk_model.<locals>.model at 0x11c1ce598>\n",
      "<function mk_model.<locals>.model at 0x11c1ce6a8>\n",
      "<function mk_model.<locals>.model at 0x11c1ce620>\n",
      "<function mk_model.<locals>.model at 0x11c1ce378>\n",
      "<function mk_model.<locals>.model at 0x11c1ce730>\n",
      "<function mk_model.<locals>.model at 0x11c1ce840>\n",
      "<function mk_model.<locals>.model at 0x11c1ce8c8>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apurvabadithela/Documents/bbopt/TreeSearch/multipolyfit/multipolyfit/core.py:66: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  beta = linalg.lstsq(A, y)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function mk_model.<locals>.model at 0x11c1ce9d8>\n",
      "<function mk_model.<locals>.model at 0x11c1ceae8>\n",
      "<function mk_model.<locals>.model at 0x11c1ce950>\n",
      "<function mk_model.<locals>.model at 0x11c1ce7b8>\n",
      "<function mk_model.<locals>.model at 0x11c1cea60>\n",
      "<function mk_model.<locals>.model at 0x11c1ceb70>\n",
      "<function mk_model.<locals>.model at 0x11c1cebf8>\n",
      "<function mk_model.<locals>.model at 0x11c1ced08>\n",
      "<function mk_model.<locals>.model at 0x11c1ced90>\n",
      "<function mk_model.<locals>.model at 0x11c1cef28>\n",
      "<function mk_model.<locals>.model at 0x11c1ceea0>\n",
      "<function mk_model.<locals>.model at 0x11c1cee18>\n",
      "<function mk_model.<locals>.model at 0x11c1cec80>\n",
      "<function mk_model.<locals>.model at 0x116b4a048>\n",
      "<function mk_model.<locals>.model at 0x116b4a1e0>\n",
      "<function mk_model.<locals>.model at 0x116b238c8>\n",
      "<function mk_model.<locals>.model at 0x116b23840>\n",
      "<function mk_model.<locals>.model at 0x116b4a0d0>\n",
      "<function mk_model.<locals>.model at 0x116b4a2f0>\n",
      "<function mk_model.<locals>.model at 0x116b4a400>\n",
      "<function mk_model.<locals>.model at 0x116b4a268>\n",
      "<function mk_model.<locals>.model at 0x116b4a158>\n",
      "<function mk_model.<locals>.model at 0x116b4a488>\n",
      "<function mk_model.<locals>.model at 0x116b4a510>\n",
      "<function mk_model.<locals>.model at 0x116b4a598>\n",
      "<function mk_model.<locals>.model at 0x116b4a378>\n",
      "<function mk_model.<locals>.model at 0x116b4a6a8>\n",
      "<function mk_model.<locals>.model at 0x116b4a730>\n",
      "<function mk_model.<locals>.model at 0x116b4a7b8>\n",
      "<function mk_model.<locals>.model at 0x116b4a620>\n",
      "<function mk_model.<locals>.model at 0x116b4a8c8>\n",
      "<function mk_model.<locals>.model at 0x116b4a950>\n",
      "<function mk_model.<locals>.model at 0x116b4a9d8>\n",
      "<function mk_model.<locals>.model at 0x116b4aa60>\n",
      "<function mk_model.<locals>.model at 0x116b4aae8>\n",
      "<function mk_model.<locals>.model at 0x116b4a840>\n",
      "<function mk_model.<locals>.model at 0x116b4abf8>\n",
      "<function mk_model.<locals>.model at 0x116b4ac80>\n",
      "<function mk_model.<locals>.model at 0x116b4ab70>\n",
      "<function mk_model.<locals>.model at 0x116b4ad90>\n",
      "<function mk_model.<locals>.model at 0x116b4aea0>\n",
      "<function mk_model.<locals>.model at 0x116b4ae18>\n",
      "<function mk_model.<locals>.model at 0x116b4ad08>\n",
      "<function mk_model.<locals>.model at 0x116b4af28>\n",
      "<function mk_model.<locals>.model at 0x11c1c6048>\n",
      "<function mk_model.<locals>.model at 0x11c1c60d0>\n",
      "<function mk_model.<locals>.model at 0x11c1c61e0>\n",
      "<function mk_model.<locals>.model at 0x11c1c6268>\n",
      "<function mk_model.<locals>.model at 0x11c1c62f0>\n",
      "<function mk_model.<locals>.model at 0x11c1c6158>\n",
      "<function mk_model.<locals>.model at 0x11c1c6400>\n",
      "<function mk_model.<locals>.model at 0x11c1c6488>\n",
      "<function mk_model.<locals>.model at 0x11c1c6510>\n",
      "<function mk_model.<locals>.model at 0x11c1c66a8>\n",
      "<function mk_model.<locals>.model at 0x11c1c6620>\n",
      "<function mk_model.<locals>.model at 0x11c1c6378>\n",
      "<function mk_model.<locals>.model at 0x11c1c6730>\n",
      "<function mk_model.<locals>.model at 0x11c1c67b8>\n",
      "<function mk_model.<locals>.model at 0x11c1c6598>\n",
      "<function mk_model.<locals>.model at 0x11c1c68c8>\n",
      "<function mk_model.<locals>.model at 0x11c1c6950>\n",
      "<function mk_model.<locals>.model at 0x11c1c6840>\n",
      "<function mk_model.<locals>.model at 0x11c1c69d8>\n",
      "<function mk_model.<locals>.model at 0x11c1c6ae8>\n",
      "<function mk_model.<locals>.model at 0x11c1c6a60>\n",
      "<function mk_model.<locals>.model at 0x11c1c6b70>\n",
      "<function mk_model.<locals>.model at 0x11c1c6c80>\n",
      "<function mk_model.<locals>.model at 0x11c1c6d08>\n",
      "<function mk_model.<locals>.model at 0x11c1c6d90>\n",
      "<function mk_model.<locals>.model at 0x11c1c6e18>\n",
      "<function mk_model.<locals>.model at 0x11c1c6ea0>\n",
      "<function mk_model.<locals>.model at 0x11c1c6f28>\n",
      "<function mk_model.<locals>.model at 0x11c1c6bf8>\n",
      "<function mk_model.<locals>.model at 0x11c1e20d0>\n",
      "0.0059859395223434975\n",
      "-0.054289851514221166\n",
      "0.0\n",
      "0.0171334078666443\n",
      "0.02262829502128827\n",
      "1.0\n",
      "-0.052408764936501706\n",
      "0.0009161605163016118\n",
      "0.0\n",
      "0.03450441742561616\n",
      "0.0069489334658093815\n",
      "0.03259938281589936\n",
      "0.005037181114748379\n",
      "-0.001904376364162852\n",
      "0.027882830598859876\n",
      "0.015501298496157778\n",
      "0.003221277090965202\n",
      "0.0439658701800415\n",
      "0.028188387234842294\n",
      "0.030054706989981805\n",
      "0.02592529823350176\n",
      "-0.04234841021872171\n",
      "0.0257551340712181\n",
      "0.009295088910679206\n",
      "-0.02085622826426392\n",
      "0.004620461423189836\n",
      "0.010308689797774773\n",
      "0.037844388462393914\n",
      "0.009426282907153083\n",
      "0.02219892794663325\n",
      "0.012584489210116791\n",
      "0.015563966752451306\n",
      "0.017288386617487106\n",
      "0.02289532685895479\n",
      "-0.008602305838615543\n",
      "-0.002660361079543261\n",
      "0.005003943276613687\n",
      "0.0165677745488384\n",
      "0.021026381185887066\n",
      "0.02156039353941608\n",
      "0.0155271700956785\n",
      "0.015501378364019214\n",
      "0.02357173223081221\n",
      "0.034856715849653885\n",
      "0.027354957965346705\n",
      "0.044575082078453176\n",
      "0.01772895762521043\n",
      "0.02170770242601905\n",
      "0.02007968403111904\n",
      "0.02596719913974022\n",
      "-0.00041156999111025\n",
      "0.015652348524695436\n",
      "0.014607438715729337\n",
      "0.024885222484003933\n",
      "0.01999737381687383\n",
      "0.024729505245077665\n",
      "0.018409004394551176\n",
      "0.016361593812180275\n",
      "0.015814516130153056\n",
      "0.026471342915795115\n",
      "0.0037474383491373775\n",
      "0.021958821724164745\n",
      "0.03331334075121306\n",
      "0.021053324836191822\n",
      "0.016482463059338315\n",
      "0.012369941748043178\n",
      "0.019920024503606082\n",
      "0.014453913257608253\n",
      "0.025910569994602728\n",
      "0.01310116472301226\n",
      "0.016965640668963\n",
      "0.009852374663897878\n",
      "0.016253606157585976\n",
      "0.014914566654972925\n",
      "0.015134154543874234\n",
      "0.021601499781741085\n",
      "0.030268042770186906\n",
      "0.017096475112710396\n",
      "0.01274632173154222\n",
      "0.02167661576079971\n",
      "0.03138205990249088\n",
      "0.027171520141570316\n",
      "0.014945616299951836\n",
      "0.014980523556374137\n",
      "0.021874159198896237\n",
      "0.01595424527954037\n",
      "0.019552986366902392\n",
      "0.026832662878456315\n",
      "0.019266347120064253\n",
      "0.016440943364237404\n",
      "0.017124961999230193\n",
      "0.033568394886853925\n",
      "0.02281064920758369\n",
      "0.01998656488736561\n",
      "0.019961816960805456\n",
      "0.028262637371923697\n",
      "0.021966040726387304\n",
      "0.02257356904427266\n",
      "0.02192709461844733\n",
      "0.020967557062799624\n"
     ]
    }
   ],
   "source": [
    "#Calling functions to construct the models:\n",
    "construct_approx_polyfit()\n",
    "construct_approx_logreg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing function approximations of the robust satisfaction value at multiple fidelities:\n",
    "# Use the polyfit function to create approximations:\n",
    "# Using discrete fidelity bounds of 1-D fidelity: z = [0.0 0.1, 0.2, 0.3, ...., 0.9]. z \\in [0,0.1) is evaluated at fidelity 0.0. \n",
    "# z = 1 is fidelity at ground truth value, and we run the simulator for that. not given with the approximations\n",
    "def find_z_closest(z, fidel_dim):\n",
    "    assert(len(z)==fidel_dim)\n",
    "    z1 = 0 # Normalized fidelity for x0\n",
    "    z2 = 0 # Normalized fidelity for  v0\n",
    "    for ii in range(1, len(disc_z)):\n",
    "        if(z[0]>=disc_z[ii-1] and z[0]<disc_z[ii]):\n",
    "            z1 = disc_z[ii-1]\n",
    "        if(z[1]>=disc_z[ii-1] and z[1]<disc_z[ii]):\n",
    "            z2 = disc_z[ii-1]\n",
    "    if(z[0] == 0.9):\n",
    "        z1 = 0.9\n",
    "    if(z[1] == 0.9):\n",
    "        z2 = 0.9\n",
    "    return z1, z2\n",
    "\n",
    "def get_approx_polyfit(approx_id):\n",
    "    rsv_approxs = rsv_approxs_polyfit.copy()\n",
    "    return rsv_approxs[approx_id]\n",
    "\n",
    "def get_approx_logreg(approx_id):\n",
    "    rsv_approxs = rsv_approxs_logreg.copy()\n",
    "    return rsv_approxs[approx_id]\n",
    "\n",
    "def retrieve_approximation(z, fidel_dim, approx_type):\n",
    "    z1, z2 = find_z_closest(z, fidel_dim)\n",
    "    if (z1 < 1):\n",
    "        z1_id = z1\n",
    "    if (z2 < 1):\n",
    "        z2_id = z2\n",
    "    approx_id = int(zidx(z1,z2))\n",
    "    # print(\"approx id: \"+str(approx_id))\n",
    "    if (approx_type == \"logreg\"):\n",
    "        approximation = get_approx_logreg(approx_id)\n",
    "    if (approx_type == \"polyfit\"):\n",
    "        approximation = get_approx_polyfit(approx_id)\n",
    "    return approximation\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high fidelity model\n",
    "def query_sim(x):\n",
    "    xarr = convert_to_arr_shape(x)\n",
    "    x0 = float(xarr[0,0])\n",
    "    v0 = float(xarr[0,1])\n",
    "    rsv = eng.query_simulator(x0, v0)\n",
    "    return -rsv\n",
    "def rsv_function(z,x, approx_type):\n",
    "    # \"\"\" Computes the rsv function. \"\"\"\n",
    "    func_computation = 0\n",
    "    if(approx_type == \"logreg\"):\n",
    "        func_computation = rsv_function_logreg(x,z)\n",
    "    if (approx_type == \"polyfit\"):\n",
    "        func_computation = rsv_function_polyfit(x,z)\n",
    "    return -func_computation\n",
    "\n",
    "def rsv_function_logreg(x, z):\n",
    "    #\"\"\" Alternative form for the rsv function. \"\"\"\n",
    "    z_arr = convert_to_arr_shape(z)\n",
    "    x_arr = convert_to_arr_shape(x)\n",
    "    approximation = retrieve_approximation(z_arr[0], fidel_dim, \"logreg\")\n",
    "    func_computation = approximation.predict(x_arr)\n",
    "    return func_computation\n",
    "\n",
    "def rsv_function_polyfit(x, z):\n",
    "    #\"\"\" Alternative form for the branin function. \"\"\"\n",
    "    approximation = retrieve_approximation(z, fidel_dim, \"polyfit\")\n",
    "    return approximation\n",
    "\n",
    "def get_mf_rsv_function(fidel_dim, approx_type):\n",
    "    #\"\"\" Returns the rsv function as a multifidelity function. \"\"\n",
    "    \n",
    "    def mf_rsv_obj(x, z):\n",
    "    #\"\"\" Wrapper for the MF rsv objective.\"\"\" \n",
    "        assert len(z) == fidel_dim\n",
    "        if (z[0]==1 or z[1]==1):\n",
    "            return query_sim(x)\n",
    "        else:\n",
    "            if approx_type == \"polyfit\":\n",
    "                return rsv_function(z,x,\"polyfit\")\n",
    "            else:\n",
    "                return rsv_function(z,x,\"logreg\")\n",
    "    \n",
    "    # Other data\n",
    "    opt_fidel = np.ones((fidel_dim)) # For now, exclude the ground truth\n",
    "    fidel_bounds = [[0, 1]] * fidel_dim\n",
    "    opt_pt = np.array([-0.52, -0.0297])\n",
    "    domain_bounds = [[-1, 0.6], [-0.42, 0.42]] # for x = [x0_bound, v0_bound]\n",
    "    return mf_rsv_obj, opt_pt, opt_fidel, fidel_bounds, domain_bounds\n",
    "\n",
    "# Function to get cost function:\n",
    "def _get_mf_cost_function(fidel_bounds, is_0_1):\n",
    "    #\"\"\" Returns the cost function. fidel_bounds are the bounds for the fidelity space\n",
    "    #  and is_0_1 should be true if fidel_bounds is [0,1]^p. \"\"\"\n",
    "    fidel_dim = len(fidel_bounds)\n",
    "    if fidel_dim == 1:\n",
    "        fidel_powers = [2]\n",
    "    elif fidel_dim == 2:\n",
    "        fidel_powers = [3, 2]\n",
    "    elif fidel_dim == 3:\n",
    "        fidel_powers = [3, 2, 1.5]\n",
    "    else:\n",
    "        fidel_powers = [3] + list(np.linspace(2, 1.2, fidel_dim-1))\n",
    "    # Define the normalised\n",
    "    def _norm_cost_function(norm_z):\n",
    "    # The cost function with normalised coordinates. \"\"\"\n",
    "        min_cost = 0.05\n",
    "        return min_cost + (1-min_cost) * np.power(norm_z, fidel_powers).sum()\n",
    "    \n",
    "    # Now return based on whether or not is_0_1\n",
    "    ret = (_norm_cost_function if is_0_1 else\n",
    "           lambda z: _norm_cost_function(map_to_cube(z, fidel_bounds)))\n",
    "    return ret\n",
    "\n",
    "# Function to convert to array that can be fed into regressor from list:\n",
    "def convert_to_arr_shape(opt_val):\n",
    "    opt_val_arr = np.array(opt_val).reshape(1,-1)\n",
    "    return opt_val_arr\n",
    "def get_mf_rsv_as_mfof(fidel_dim, approx_type):\n",
    "#\"\"\" Wrapper for get_mf_rsv_function which returns as a mfof. \"\"\"\n",
    "    mf_rsv_obj, opt_pt, opt_fidel, fidel_bounds, domain_bounds = get_mf_rsv_function(fidel_dim, approx_type)\n",
    "    fidel_cost_function = _get_mf_cost_function(fidel_bounds, True) # Figure out this line\n",
    "\n",
    "    opt_val = mf_rsv_obj(opt_fidel, opt_pt)\n",
    "    return MFOptFunction(mf_rsv_obj, fidel_cost_function, fidel_bounds, domain_bounds,\n",
    "                       opt_fidel, vectorised=False, opt_pt=opt_pt, opt_val=opt_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0\n",
      "[0.070874]\n",
      "[0.04495125]\n",
      "[0.03591332]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 1: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 2: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 3: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 4: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 5: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 6: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 7: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 8: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 9: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 10: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 11: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 12: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 13: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 14: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 15: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 16: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 17: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 18: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 19: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 20: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 21: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 22: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 23: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 24: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 25: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 26: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 27: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 28: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 29: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 30: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 31: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 32: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 33: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 34: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 35: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 36: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 37: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 38: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 39: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 40: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 41: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 42: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 43: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 44: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 45: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 46: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 47: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 48: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 49: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 50: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 51: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 52: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 53: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 54: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 55: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 56: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 57: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 58: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 59: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 60: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 61: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 62: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 63: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 64: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 65: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 66: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 67: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 68: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 69: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 70: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 71: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 72: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 73: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 74: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 75: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 76: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 77: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 78: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 79: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 80: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 81: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 82: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 83: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 84: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 85: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 86: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 87: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 88: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 89: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 90: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 91: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 92: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 93: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 94: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 95: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 96: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 97: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 98: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False),\n",
       " 99: SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing RSV function:\n",
    "fidel_dim = 2\n",
    "mf_rsv_obj, opt_pt, opt_fidel, fidel_bounds, domain_bounds = get_mf_rsv_function(fidel_dim, \"logreg\")\n",
    "z1 = [0.1, 0.1]\n",
    "x1 = [0.1, 0.2]\n",
    "z2 = [-0.5, -0.3]\n",
    "x2 = [0.1, 0.2]\n",
    "xopt = [-0.52, -0.0297]\n",
    "zopt = [1,1]\n",
    "\n",
    "# print(mf_rsv_obj(xopt, zopt)) # sanity check; should be 0.972\n",
    "ztest1 = [0.1, 0.1]\n",
    "zid1 = zidx(ztest1[0], ztest1[1])\n",
    "print(zid1)\n",
    "print(mf_rsv_obj(xopt, ztest1)) # \n",
    "ztest2 = [0.1, 0.9]\n",
    "print(mf_rsv_obj(xopt, ztest2)) # \n",
    "ztest3 = [0.9, 0.9]\n",
    "print(mf_rsv_obj(xopt, ztest3)) # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using MF Optimization:\n",
    "mf_rsv_opt = get_mf_rsv_as_mfof(fidel_dim, \"logreg\")\n",
    "noise_var = 0\n",
    "sigma = np.sqrt(noise_var)\n",
    "\n",
    "# Running a single experiment:\n",
    "NUM_EXP = 1\n",
    "EXP_NAME = \"RSV_Mountain_Car\"\n",
    "def run_one_experiment(mfobject,nu,rho,times,sigma,C,t0,filename):\n",
    "    R = []\n",
    "    T = []\n",
    "    Xarr = []\n",
    "    for t in times:\n",
    "        budget = t*mfobject.opt_fidel_cost\n",
    "        t1 = time.time()\n",
    "        MP = MFPOO(mfobject=mfobject, nu_max=nu, rho_max=rho, total_budget=budget, sigma=sigma, \n",
    "                   C=C, mult=0.5, tol = 1e-3, Randomize = False, Auto = True, unit_cost=t0 )\n",
    "        MP.run_all_MFHOO()\n",
    "        X, E = MP.get_point()\n",
    "        t2 = time.time()\n",
    "\n",
    "        R = R + [E]\n",
    "        T = T + [MP.cost]\n",
    "        Xarr = Xarr + [X]\n",
    "    print(str(MP.cost) + ' : ' + str(E))\n",
    "    #print 'Total HOO Queries: ' + str(MP.t) \n",
    "    np.save(filename,R)\n",
    "    return np.array(R),np.array(T), np.array(Xarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = -1\n",
    "b = 0.6\n",
    "c = -0.42\n",
    "d = 0.42\n",
    "rho_analysis = 0.25\n",
    "nu_analysis = 4*(b-a)**2 + 4*(d-c)**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Experiment 1: \n",
      "Auto Init: \n",
      "C: 0.009300477687407029\n",
      "nu: 0.009300477687407029\n",
      "Budget Remaining: 18.25965691566467\n",
      "Number of MFHOO Instances: 5\n",
      "Budget per MFHOO Instance:1.7019313831329341\n",
      "Running SOO number: 1 rho: 0.95 nu: 0.009300477687407029\n",
      "Done!\n",
      "Running SOO number: 2 rho: 0.9378956176563621 nu: 0.009300477687407029\n",
      "Done!\n",
      "Updating C\n",
      "C: 0.01302066876236984\n",
      "nu_max: 0.01302066876236984\n",
      "Running SOO number: 3 rho: 0.9180634032924686 nu: 0.01302066876236984\n",
      "Done!\n",
      "Updating C\n",
      "C: 0.018228936267317773\n",
      "nu_max: 0.018228936267317773\n",
      "Running SOO number: 4 rho: 0.8796481896190089 nu: 0.018228936267317773\n",
      "Done!\n",
      "Updating C\n",
      "C: 0.02552051077424488\n",
      "nu_max: 0.02552051077424488\n",
      "Running SOO number: 5 rho: 0.7737809374999998 nu: 0.02552051077424488\n",
      "Done!\n",
      "Updating C\n",
      "C: 0.035728715083942826\n",
      "nu_max: 0.035728715083942826\n",
      "Auto Init: \n",
      "C: 0.009300477687407029\n",
      "nu: 0.009300477687407029\n",
      "Budget Remaining: 37.759758243560796\n",
      "Number of MFHOO Instances: 10\n",
      "Budget per MFHOO Instance:1.8259758243560795\n",
      "Running SOO number: 1 rho: 0.95 nu: 0.009300477687407029\n",
      "Done!\n",
      "Running SOO number: 2 rho: 0.9446011072613543 nu: 0.009300477687407029\n",
      "Done!\n",
      "Updating C\n",
      "C: 0.01302066876236984\n",
      "nu_max: 0.01302066876236984\n",
      "Running SOO number: 3 rho: 0.9378956176563621 nu: 0.01302066876236984\n",
      "Done!\n",
      "Updating C\n",
      "C: 0.018228936267317773\n",
      "nu_max: 0.018228936267317773\n",
      "Running SOO number: 4 rho: 0.9293441702879052 nu: 0.018228936267317773\n",
      "Done!\n",
      "Updating C\n",
      "C: 0.02552051077424488\n",
      "nu_max: 0.02552051077424488\n",
      "Running SOO number: 5 rho: 0.9180634032924686 nu: 0.02552051077424488\n",
      "Done!\n",
      "Updating C\n",
      "C: 0.035728715083942826\n",
      "nu_max: 0.035728715083942826\n",
      "Running SOO number: 6 rho: 0.9025 nu: 0.035728715083942826\n",
      "Done!\n",
      "Updating C\n",
      "C: 0.05002020111751995\n",
      "nu_max: 0.05002020111751995\n",
      "Running SOO number: 7 rho: 0.8796481896190089 nu: 0.05002020111751995\n",
      "Done!\n",
      "Updating C\n",
      "C: 0.07002828156452792\n",
      "nu_max: 0.07002828156452792\n",
      "Running SOO number: 8 rho: 0.8428404124649498 nu: 0.07002828156452792\n",
      "Done!\n",
      "Running SOO number: 9 rho: 0.7737809374999998 nu: 0.07002828156452792\n",
      "Done!\n",
      "Running SOO number: 10 rho: 0.5987369392383787 nu: 0.07002828156452792\n",
      "Done!\n",
      "Auto Init: \n",
      "C: 0.009300477687407029\n",
      "nu: 0.009300477687407029\n",
      "Budget Remaining: 96.25984216690063\n",
      "Number of MFHOO Instances: 16\n",
      "Budget per MFHOO Instance:4.066240135431289\n",
      "Running SOO number: 1 rho: 0.95 nu: 0.009300477687407029\n",
      "Done!\n",
      "Running SOO number: 2 rho: 0.9467569727007391 nu: 0.009300477687407029\n",
      "Done!\n",
      "Updating C\n",
      "C: 0.01302066876236984\n",
      "nu_max: 0.01302066876236984\n",
      "Running SOO number: 3 rho: 0.943064209619936 nu: 0.01302066876236984\n",
      "Done!\n",
      "Updating C\n",
      "C: 0.018228936267317773\n",
      "nu_max: 0.018228936267317773\n",
      "Running SOO number: 4 rho: 0.9388212232486434 nu: 0.018228936267317773\n",
      "Done!\n",
      "Updating C\n",
      "C: 0.02552051077424488\n",
      "nu_max: 0.02552051077424488\n",
      "Running SOO number: 5 rho: 0.9338951938669805 nu: 0.02552051077424488\n",
      "Done!\n",
      "Updating C\n",
      "C: 0.035728715083942826\n",
      "nu_max: 0.035728715083942826\n",
      "Running SOO number: 6 rho: 0.9281068356493957 nu: 0.035728715083942826\n",
      "Done!\n",
      "Updating C\n",
      "C: 0.05002020111751995\n",
      "nu_max: 0.05002020111751995\n",
      "Running SOO number: 7 rho: 0.9212081434582966 nu: 0.05002020111751995\n",
      "Done!\n",
      "Updating C\n",
      "C: 0.07002828156452792\n",
      "nu_max: 0.07002828156452792\n",
      "Running SOO number: 8 rho: 0.912846007669677 nu: 0.07002828156452792\n",
      "Done!\n",
      "Running SOO number: 9 rho: 0.9025 nu: 0.07002828156452792\n",
      "Done!\n",
      "Running SOO number: 10 rho: 0.8893701034660746 nu: 0.07002828156452792\n",
      "Done!\n",
      "Running SOO number: 11 rho: 0.8721602331278452 nu: 0.07002828156452792\n",
      "Done!\n",
      "Running SOO number: 12 rho: 0.8486244435738817 nu: 0.07002828156452792\n",
      "Done!\n",
      "Running SOO number: 13 rho: 0.8145062499999999 nu: 0.07002828156452792\n",
      "Done!\n",
      "Running SOO number: 14 rho: 0.7606634722496172 nu: 0.07002828156452792\n",
      "Done!\n",
      "Running SOO number: 15 rho: 0.6634204312890623 nu: 0.07002828156452792\n",
      "Done!\n",
      "Running SOO number: 16 rho: 0.44012666865176536 nu: 0.07002828156452792\n",
      "Done!\n",
      "96.26255578994751 : [0.001793743893962068, 0.009171422941205131, 0.027129576656401005, 0.027129576656401005, 0.027129576656401005, 0.027129576656401005, 0.027129576656401005, 0.027129576656401005, 0.027129576656401005, 0.027129576656401005, 0.027129576656401005, 0.027129576656401005, 0.027129576656401005, 0.027129576656401005, 0.027129576656401005, 0.009512139598874716]\n",
      "Finished eXP 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apurvabadithela/opt/anaconda3/envs/falsification/lib/python3.6/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n",
      "/Users/apurvabadithela/opt/anaconda3/envs/falsification/lib/python3.6/site-packages/ipykernel_launcher.py:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-00d8a0307cb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmfobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_val\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/Users/apurvabadithela/opt/anaconda3/envs/falsification/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3372\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3373\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apurvabadithela/opt/anaconda3/envs/falsification/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "# Running experiments:\n",
    "# times = [10,20,50,75,100,150,175, 200]\n",
    "# mfobject = get_noisy_mfof_from_mfof(mf_rsv_opt, noise_var)\n",
    "mfobject = mf_rsv_opt\n",
    "times = [10, 20, 50]\n",
    "nu = nu_analysis\n",
    "rho = 0.95\n",
    "C = 0.1\n",
    "t0 = mfobject.opt_fidel_cost\n",
    "\n",
    "\n",
    "NT = str(time.time())\n",
    "print('Running Experiment 1: ')\n",
    "filename = 'MFHOO' + EXP_NAME + '_' + NT + '_' + '1.npy'\n",
    "R,T, Xarr = run_one_experiment(mfobject,nu,rho,times,sigma,C,t0,filename)\n",
    "result = R\n",
    "\n",
    "print('Finished eXP 1')\n",
    "for i in range(1,NUM_EXP):\n",
    "    print('Running Experiment' + str(i+1) + ': ')\n",
    "    filename = 'MFHOO' + EXP_NAME + '_' + NT + '_' + str(i+1) + '.npy'\n",
    "    R,T, Xarr = run_one_experiment(mfobject,nu,rho,times,sigma,C,t0,filename)\n",
    "    result = np.vstack([result,R])\n",
    "\n",
    "\n",
    "# df = pd.DataFrame(dfdic)\n",
    "# df.to_csv(filename) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(result,axis = 0)\n",
    "std = np.std(result,axis = 0)\n",
    "result = mfobject.opt_val - mu\n",
    "\n",
    "filename = './data/mountain_car/MFHOO_' + EXP_NAME + '_' + NT + '_' + '.csv'\n",
    "dfdic = {}\n",
    "dfdic['Capital'] = np.array(times)\n",
    "dfdic['Value'] = result\n",
    "dfdic['Std'] = std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dfdic, open('SGDRegressor_t_50', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([array([ 0.378125  , -0.40523437]), array([ 0.203125  , -0.41835937]), array([0.10078125, 0.31541016]), array([0.10078125, 0.31766602]), array([0.10078125, 0.31602539])]),\n",
       "       list([array([ 0.303125  , -0.41917969]), array([ 0.303125  , -0.41507812]), array([0.10078125, 0.31541016]), array([0.10234375, 0.31520508]), array([0.10390625, 0.31520508]), array([0.10234375, 0.31520508]), array([0.10078125, 0.31541016]), array([0.10078125, 0.30228516]), array([0.10078125, 0.31602539]), array([0.10078125, 0.31520508])]),\n",
       "       list([array([ 0.303125  , -0.41753906]), array([ 0.403125  , -0.41917969]), array([0.10078125, 0.31520508]), array([0.10039062, 0.31561523]), array([0.10039062, 0.31561523]), array([0.10234375, 0.30208008]), array([0.10078125, 0.30208008]), array([0.10078125, 0.30228516]), array([0.10078125, 0.30208008]), array([0.10078125, 0.30208008]), array([0.10078125, 0.30208008]), array([0.10078125, 0.30208008]), array([0.10507813, 0.31602539]), array([0.10039062, 0.31561523]), array([0.10039062, 0.31520508]), array([-0.9984375 ,  0.10541016])])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result\n",
    "Xarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mf.mf_func import NoisyMFOptFunction, plot_2d_function\n",
    "\n",
    "# def visualise_mfof(mfof):\n",
    "#     \"\"\" Visualises the mfof object. \"\"\"\n",
    "#     plot_func = mfof.eval_multiple\n",
    "#     _, ax, plt = plot_2d_function(plot_func,\n",
    "#                                np.array([mfof.fidel_bounds[0], mfof.domain_bounds[0]]),\n",
    "#                                x_label='fidel', y_label='domain')\n",
    "#     ax.scatter(mfof.opt_fidel, mfof.opt_pt, mfof.opt_val, c='r', s=100)\n",
    "#     plt.show()\n",
    "\n",
    "# visualise_mfof(mf_rsv_opt) # visualize mfof function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(\"MFHOORSV_Mountain_Car_1597098498.912148_1.npy\")\n",
    "T\n",
    "opt_val = mf_rsv_obj(opt_fidel, opt_pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quit the MATLAB engine:\n",
    "eng.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
